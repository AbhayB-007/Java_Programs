<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Quarkus 2.6.1.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-6-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-6-1-final-released/</id><updated>2021-12-24T00:00:00Z</updated><content type="html">2021 has been a tremendous year for Quarkus and today we officially close our 2021 release season with the last maintenance release of the year: 2.6.1.Final is available on Maven Central and ready for consumption! It is a safe upgrade for anyone already using 2.6 and contains dozens of small...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>More machine learning with OpenShift Data Science</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/23/more-machine-learning-openshift-data-science" /><author><name>Audrey Reznik</name></author><id>749ff7cc-63c2-4b3c-b5bc-d8f080295789</id><updated>2021-12-23T07:00:00Z</updated><published>2021-12-23T07:00:00Z</published><summary type="html">&lt;p&gt;We hope you enjoyed the first two &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt; learning paths: &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/launch-red-hat-openshift-data-science"&gt;Launch Red Hat OpenShift Data Science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/red-hat-openshift-data-science-resources"&gt;OpenShift Data Science documentation and resources&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This week, we've released two new learning paths, which address the common &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; challenges of &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/access-download-analyze-s3-data"&gt;accessing Amazon S3 data&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/create-a-tensorflow-model"&gt;creating a TensorFlow model&lt;/a&gt;. Developers and data scientists can use these hands-on courses to learn how to access data and create machine learning models. You'll also learn how much easier common data science procedures are with OpenShift Data Science.&lt;/p&gt; &lt;p&gt;This article introduces the learning paths and provides an overview of OpenShift Data Science, including where to find more information.&lt;/p&gt; &lt;h2&gt;Accessing Amazon S3 data with OpenShift Data Science&lt;/h2&gt; &lt;p&gt;In this learning path you will learn how to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Set up your JupyterHub image to use Amazon S3.&lt;/li&gt; &lt;li&gt;Access and download Amazon S3 data from Amazon Web Services (AWS).&lt;/li&gt; &lt;li&gt;Analyze your Amazon S3 data using &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; data frames.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/access-download-analyze-s3-data"&gt;Start the learning path&lt;/a&gt; now.&lt;/p&gt; &lt;h2&gt;Creating TensorFlow models in OpenShift Data Science&lt;/h2&gt; &lt;p&gt;In this learning path you will learn how to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Set up your JupyterHub image to use TensorFlow.&lt;/li&gt; &lt;li&gt;Explore a large public data set (MNIST).&lt;/li&gt; &lt;li&gt;Build, train, and test a TensorFlow model.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/getting-started/create-a-tensorflow-model"&gt;Start the learning path&lt;/a&gt; now.&lt;/p&gt; &lt;p&gt;Remember that you can also try OpenShift Data Science in the &lt;a href="https://developers.redhat.com/developer-sandbox/activities"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Visit the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science"&gt;OpenShift Data Science&lt;/a&gt; page to see our complete library of learning paths and other resources for developers and data scientists collaborating on intelligent applications.&lt;/p&gt; &lt;h2&gt;What is OpenShift Data Science?&lt;/h2&gt; &lt;p&gt;OpenShift Data Science is a platform that makes it easier for developers and data scientists to develop, deploy, and monitor &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; models. As a comprehensive environment built on top of &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;, OpenShift Data Science integrates Jupyter notebooks—the core IDE where data scientists train models—with model development frameworks such as TensorFlow and PyTorch.&lt;/p&gt; &lt;p&gt;You can think of OpenShift Data Science as a meta-operator that sits above other &lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Kubernetes Operators&lt;/a&gt; and combines them into a coherent, integrated environment. Currently, OpenShift Data Science partner technologies include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Anaconda Commercial Edition for secure distribution and package management&lt;/li&gt; &lt;li&gt;IBM Watson Studio for building and managing models at scale and for AutoML&lt;/li&gt; &lt;li&gt;Intel OpenVINO and oneAPI AI analytics toolkits for optimizing and tuning models&lt;/li&gt; &lt;li&gt;Seldon Deploy for deploying, managing, and monitoring models&lt;/li&gt; &lt;li&gt;Starburst Galaxy for data integration&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Support for NVIDIA accelerated computing is coming very soon!&lt;/p&gt; &lt;h2&gt;Where can I learn more?&lt;/h2&gt; &lt;p&gt;Visit the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science"&gt;OpenShift Data Science&lt;/a&gt; landing page to learn more about how data scientists, data engineers, and application developers use this service to collaborate across the intelligent application life cycle.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/23/more-machine-learning-openshift-data-science" title="More machine learning with OpenShift Data Science"&gt;More machine learning with OpenShift Data Science&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Audrey Reznik</dc:creator><dc:date>2021-12-23T07:00:00Z</dc:date></entry><entry><title type="html">How to run WildFly on Openshift</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=using-wildfly-on-openshift" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=using-wildfly-on-openshift</id><updated>2021-12-23T01:38:00Z</updated><content type="html">This tutorial will teach you how to run the latest version of WildFly on Openshift. We will then look at advanced options such as overriding the default server settings (WildFly 26 update). Finally, we will learn how to create a custom WildFly distribution using Galleon layers. Setting up WildFly Image Streams Openshift uses Image Streams ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Important security vulnerability discovered</title><link rel="alternate" href="https://www.keycloak.org/2021/12/cve" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2021/12/cve</id><updated>2021-12-23T00:00:00Z</updated><content type="html">A flaw () was found in Keycloak version from 12.0.0 and before 15.1.1 which allows an attacker with any existing user account to create new default user accounts via the administrative REST API even when new user registration is disabled. In most situations the newly created user is the equivalent of a self-registered user, and does not have the ability to receive any additional roles or groups. However, there are some vectors that are harder to reproduce, but may result in additional privileges. We highly recommend everyone upgrade to Keycloak 15.1.1 or 16.1.0 as soon as possible. Keycloak 16.0.0 also includes the fix, but if you are not already running this version we recommend going straight to 16.1.0. If you are unable to upgrade we recommend mitigate the issue by blocking access to the user creation REST endpoint in the interim. This can be achieved with the following CLI commands: bin/jboss-cli.sh --connect /subsystem=undertow/configuration=filter/expression-filter=keycloakPathOverrideUsersCreateEndpoint:add( \ expression="(regex('^/auth/admin/realms/(.*)/users$') and method(POST))-&gt; response-code(400)" \ ) /subsystem=undertow/server=default-server/host=default-host/filter-ref=keycloakPathOverrideUsersCreateEndpoint:add() This will block both valid and invalid attempts at creating new users, including requests from the Keycloak admin console. Alternatively, the path /auth/admin/realms/.*/users and method POST, or /auth/admin completely, can be blocked with a firewall. For more information about the flaw please view and .</content><dc:creator>Stian Thorgersen</dc:creator></entry><entry><title>How to use Quarkus with the Service Binding Operator</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator" /><author><name>Ioannis Kanellos</name></author><id>3392814a-3a5d-44ed-987f-7562c22d88d2</id><updated>2021-12-22T07:00:00Z</updated><published>2021-12-22T07:00:00Z</published><summary type="html">&lt;p&gt;In the seven years since &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; was released, there have been various efforts to simplify the process of consuming and binding to services from Kubernetes clusters. While discovering a service isn't much of an issue if you employ a well-known set of conventions, getting the credentials and other details required to access that service is sometimes trickier.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://svc-cat.io"&gt;Kubernetes Service Catalog&lt;/a&gt; was an attempt to simplify provisioning and binding to services, but it seems to have lost momentum. The lack of uniformity between providers, differences in how each service communicates binding information, and the fact that developers tend to favor operators for provisioning services all made the Service Catalog hard to use in practice.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; for Kubernetes and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; is a more recent initiative. It stays out of the way of service provisioning, leaving that to operators. Instead, it focuses on how to best communicate binding information to the application. An interesting part of the specification is the &lt;a href="https://github.com/servicebinding/spec#workload-projection"&gt;workload projection&lt;/a&gt;, which defines a directory structure that will be mounted to the application container when binding occurs in order to pass all the required binding information: type, URI, and credentials&lt;/p&gt; &lt;p&gt;Other parts of the specification are related to the &lt;code&gt;ServiceBinding&lt;/code&gt; resource, which controls which services are bound to which application, and how.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;, which already supports the workload-projection part of the Service Binding specification, recently received enhancements for service binding. In this article, you'll learn how to automatically generate a &lt;code&gt;ServiceBinding&lt;/code&gt; resource, then go through the whole process from installing operators to configuring and deploying an application.&lt;/p&gt; &lt;h2&gt;A note about the example&lt;/h2&gt; &lt;p&gt;In the example, you will use &lt;a href="https://kind.sigs.k8s.io/"&gt;kind&lt;/a&gt; to install the Service Binding Operator and the &lt;a href="https://github.com/CrunchyData/postgres-operator"&gt;Postgres Operator from Crunchy Data&lt;/a&gt;. After that, you will create a PostgreSQL cluster, and finally create a simple todo application, deploying it and binding it to the provisioned cluster. Before you begin, you may want to take a look at the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/getting-started/quick-start.html"&gt;Service Binding Operator Quick Start Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Set up your clusters&lt;/h2&gt; &lt;p&gt;Begin by creating a new cluster with &lt;code&gt;kind&lt;/code&gt;. (If you've already created one, or don't use &lt;code&gt;kind&lt;/code&gt; at all, you can skip this step.)&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kind create cluster&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll install both of the operators used in our example through the &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Install the Operator Lifecycle Manager&lt;/h3&gt; &lt;p&gt;The first step is to install the &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.19.1/install.sh | bash -s v0.19.1 &lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Install the Service Binding Operator&lt;/h3&gt; &lt;p&gt;Next, you'll install the Service Binding Operator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create -f https://operatorhub.io/install/service-binding-operator.yaml&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Before moving to the next step, verify the installation with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get csv -n operators -w&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When the &lt;code&gt;phase&lt;/code&gt; of the Service Binding Operator is &lt;code&gt;Succeeded&lt;/code&gt;, you can proceed.&lt;/p&gt; &lt;h3&gt;Install the Postgres Operator&lt;/h3&gt; &lt;p&gt;Use the following command to install the Postgres Operator from Crunchy Data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create -f https://operatorhub.io/install/postgresql.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you did before, you'll want to verify the installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get csv -n operators -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the &lt;code&gt;phase&lt;/code&gt; of the operator is &lt;code&gt;Succeeded&lt;/code&gt;, you can move to the next stage.&lt;/p&gt; &lt;h2&gt;Create a PostgreSQL cluster&lt;/h2&gt; &lt;p&gt;To begin this process, create a new namespace where you'll install your cluster and application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create ns demo kubectl config set-context --current --namespace=demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create the cluster, you need to apply the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; apiVersion: postgres-operator.crunchydata.com/v1beta1 kind: PostgresCluster metadata: name: pg-cluster namespace: demo spec: image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres-ha:centos8-13.4-0 postgresVersion: 13 instances: - name: instance1 dataVolumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi backups: pgbackrest: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.33-2 repos: - name: repo1 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi - name: repo2 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi proxy: pgBouncer: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbouncer:centos8-1.15-2 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This resource has been borrowed from the &lt;em&gt;Service Binding Operator Quick Start Guide&lt;/em&gt;. Save that file as &lt;code&gt;pg-cluster.yml&lt;/code&gt; and apply it using &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f ~/pg-cluster.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now check the pods to verify the installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get pods -n demo&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the Quarkus application&lt;/h2&gt; &lt;p&gt;Next, you'll create a simple Quarkus todo application that will connect to PostgreSQL via Hibernate and Panache. The todo application is a simple rest API for creating, reading, and deleting todo entries in a PostgreSQL database. It is heavily inspired by Clement Escoffier's &lt;a href="https://github.com/cescoffier/quarkus-todo-app"&gt;Quarkus todo app&lt;/a&gt; but focuses less on presentation and more on the binding aspect.&lt;/p&gt; &lt;p&gt;Generate the application using the following Maven command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn io.quarkus.platform:quarkus-maven-plugin:2.5.0.Final:create -DprojectGroupId=org.acme -DprojectArtifactId=todo-example -DclassName="org.acme.TodoResource" -Dpath="/todo" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, add all of the required extensions for connecting to PostgreSQL, generate the required Kubernetes resources, and build a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image for the application using Docker:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./mvnw quarkus:add-extension -Dextensions="resteasy-jackson,jdbc-postgresql,hibernate-orm-panache,kubernetes,kubernetes-service-binding,container-image-docker" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, you need to create a simple entity:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme; import javax.persistence.Column; import javax.persistence.Entity; import io.quarkus.hibernate.orm.panache.PanacheEntity; @Entity public class Todo extends PanacheEntity { @Column(length = 40, unique = true) public String title; public boolean completed; public Todo() { } public Todo(String title, Boolean completed) { this.title = title; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, expose it via REST:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme; import javax.transaction.Transactional; import javax.ws.rs.*; import javax.ws.rs.core.Response; import javax.ws.rs.core.Response.Status; import java.util.List; @Path("/todo") public class TodoResource { @GET @Path("/") public List&lt;Todo&gt; getAll() { return Todo.listAll(); } @GET @Path("/{id}") public Todo get(@PathParam("id") Long id) { Todo entity = Todo.findById(id); if (entity == null) { throw new WebApplicationException("Todo with id of " + id + " does not exist.", Status.NOT_FOUND); } return entity; } @POST @Path("/") @Transactional public Response create(Todo item) { item.persist(); return Response.status(Status.CREATED).entity(item).build(); } @GET @Path("/{id}/complete") @Transactional public Response complete(@PathParam("id") Long id) { Todo entity = Todo.findById(id); entity.id = id; entity.completed = true; return Response.ok(entity).build(); } @DELETE @Transactional @Path("/{id}") public Response delete(@PathParam("id") Long id) { Todo entity = Todo.findById(id); if (entity == null) { throw new WebApplicationException("Todo with id of " + id + " does not exist.", Status.NOT_FOUND); } entity.delete(); return Response.noContent().build(); } } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Bind to the target cluster&lt;/h2&gt; &lt;p&gt;In order to bind the PostgreSQL service to the application, you must either provide a &lt;code&gt;ServiceBinding&lt;/code&gt; resource or have it generated. To have the binding generated for you, you need to provide the following service coordinates:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt;: &lt;code&gt;postgres-operator.crunchydata.com/v1beta1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Kind: &lt;code&gt;PostgresCluster&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Name: &lt;code&gt;pg-cluster&lt;/code&gt;, prefixed with &lt;code&gt;quarkus.kubernetes-service-binding.services.&lt;id&gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can see these coordinates in the following listing:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes-service-binding.services.my-db.api-version=postgres-operator.crunchydata.com/v1beta1 quarkus.kubernetes-service-binding.services.my-db.kind=PostgresCluster quarkus.kubernetes-service-binding.services.my-db.name=pg-cluster &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the &lt;code&gt;id&lt;/code&gt; is just used to group properties together and can be any text.&lt;/p&gt; &lt;p&gt;You also need to configure the &lt;code&gt;datasource&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.datasource.db-kind=postgresql quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.sql-load-script=import.sql &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This sample application will not push the image to a registry, but just loads it to the cluster, so use &lt;code&gt;IfNotPresent&lt;/code&gt; as the image pull policy:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes.image-pull-policy=IfNotPresent&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application properties file should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes-service-binding.services.my-db.api-version=postgres-operator.crunchydata.com/v1beta1 quarkus.kubernetes-service-binding.services.my-db.kind=PostgresCluster quarkus.kubernetes-service-binding.services.my-db.name=pg-cluster quarkus.datasource.db-kind=postgresql quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.sql-load-script=import.sql quarkus.kubernetes.image-pull-policy=IfNotPresent &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Prepare for deployment&lt;/h2&gt; &lt;p&gt;Before you deploy, you need to perform a container image build, load the image to the cluster, and generate the resource.&lt;/p&gt; &lt;p&gt;First, build the container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean install -Dquarkus.container-image.build=true -DskipTests &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that this instruction assumes that you have Docker up and running.&lt;/p&gt; &lt;p&gt;Next, you'll load the Docker image to the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kind load docker-image iocanel/todo-example:1.0.0-SNAPSHOT &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If you're using &lt;a href="https://minikube.sigs.k8s.io/docs/start/"&gt;minikube&lt;/a&gt; instead of Docker, execute the following to rebuild the image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;eval $(minikube docker-env) &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When using tools like &lt;code&gt;kind&lt;/code&gt; or &lt;code&gt;minikube&lt;/code&gt;, it is generally a good idea to change the image pull policy to &lt;code&gt;IfNotPresent&lt;/code&gt; as you did in this example. Doing this avoids unnecessary pulls, because most of the time the image will be loaded from the local Docker daemon.&lt;/p&gt; &lt;h2&gt;Deploy the application&lt;/h2&gt; &lt;p&gt;Next, generate the deployment manifest, including the &lt;code&gt;ServiceBinding&lt;/code&gt;, and apply them on Kubernetes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean install -Dquarkus.kubernetes.deploy=true -DskipTests&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now, verify that everything is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get pods -n demo -w&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Verify the installation&lt;/h2&gt; &lt;p&gt;The simplest way to verify that everything works as expected is to port forward to a local HTTP port and access the &lt;code&gt;/todo&lt;/code&gt; endpoint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl port-forward service/todo-example 8080:80&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Point your browser to &lt;a href="http://localhost:8080/todo"&gt;http://localhost:8080/todo&lt;/a&gt; and enjoy!&lt;/p&gt; &lt;h2&gt;A look ahead&lt;/h2&gt; &lt;p&gt;I am very excited about recent progress on the service binding front. In the near future, we may be able to reduce the amount of configuration necessary with the use of smart conventions (such as assuming that the custom resource name is the same as the database name unless explicitly specified otherwise) and a reasonable set of defaults (such as assuming that for PostgreSQL the default operator is Crunchy Data's Postgres Operator). In such a scenario, you could bind to services with no configuration without sacrificing flexibility or customizability. I hope you are as excited as I am by this prospect!&lt;/p&gt; &lt;p&gt;See the following resources to learn more about service binding and the Service Binding Operator:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Announcing Service Binding Operator 1.0 GA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/12/20/service-binding-operator-the-operator-in-action"&gt;Service Binding Operator: The Operator in action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/03/how-use-service-binding-rabbitmq"&gt;How to use service binding with RabbitMQ&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator" title="How to use Quarkus with the Service Binding Operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ioannis Kanellos</dc:creator><dc:date>2021-12-22T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.6.0.Final released - SmallRye Reactive Messaging 3.13 and Kafka 3, programmatic API for caching, Kotlin 1.6 and a lot more!</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-6-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-6-0-final-released/</id><updated>2021-12-22T00:00:00Z</updated><content type="html">It is my great pleasure to announce the release of Quarkus 2.6.0.Final which comes with some nice changes: Some extensions moved to Quarkiverse Hub SmallRye Reactive Messaging 3.13 and Kafka 3 Programmatic API for caching Smaller image for native executables Built-in UPX compression AWT extension Kotlin 1.6 Camel 3.14 Due...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">How to handle Exceptions in JAX-RS applications</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications</id><updated>2021-12-21T10:02:32Z</updated><content type="html">This article will teach you how to handle Exceptions properly in RESTful Web services using JAX-RS API and some advanced options which are available with RESTEasy and Quarkus runtime. Overview of REST Exceptions As for every Java classes, REST Endpoints can throw in their code both checked Exceptions (i.e. classes extending java.lang.Exception) and unchecked (i.e. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Prevent Python dependency confusion attacks with Thoth</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" /><author><name>Fridolin Pokorny</name></author><id>ae654f04-c49e-4f00-acd0-060b226f40bf</id><updated>2021-12-21T07:00:00Z</updated><published>2021-12-21T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; became popular as a casual scripting language but has since evolved into the corporate space, where it is used for &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; applications, among others. Because Python is a high-level programming language, developers often use it to quickly prototype applications. &lt;a href="https://docs.python.org/3/extending/extending.html"&gt;Python native extensions&lt;/a&gt; make it easy to optimize any computation-intensive parts of the application using a lower-level programming language like &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For applications that need to scale, we can use &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python Source-to-Image tooling&lt;/a&gt; (S2I) to convert a Python application into a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image. That image can then be orchestrated and scaled using cluster orchestrators such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. All of these features together provide a convenient platform for solving problems using Python-based solutions that scale, are maintainable, and are easily extensible.&lt;/p&gt; &lt;p&gt;As a community-based project, the main source of open-source Python packages is the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; (PyPI). As of this writing, PyPI hosts more than 3 million releases, and the number of releases available continues to grow exponentially. PyPI's growth is an indicator of Python's popularity worldwide.&lt;/p&gt; &lt;p&gt;However, Python's community-driven dependency resolvers were not designed for corporate environments, and that has led to dependency management issues and vulnerabilities in the Python ecosystem. This article describes some of the risks involved in resolving Python dependencies and introduces &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;'s tools for avoiding them.&lt;/p&gt; &lt;h2&gt;Dependency management in Python&lt;/h2&gt; &lt;p&gt;The Python package installer, &lt;a href="https://pypi.org/project/pip"&gt;pip&lt;/a&gt;, is a popular tool for resolving Python application dependencies. Unfortunately, pip does not provide a way to manage lock files for application dependencies. Pip resolves dependencies to the latest possible versions at the given point in time, so the resolution is highly dependent on the time when the resolution process was triggered. Dependency problems such as &lt;em&gt;overpinning&lt;/em&gt; (requesting too wide a range of versions) frequently introduce issues to the Python application stack.&lt;/p&gt; &lt;p&gt;To address lock file management issues, the Python community developed tools such as &lt;a href="https://pypi.org/project/pip-tools/"&gt;pip-tools&lt;/a&gt;, &lt;a href="https://pipenv.pypa.io/"&gt;Pipenv&lt;/a&gt;, and &lt;a href="https://python-poetry.org/"&gt;Poetry&lt;/a&gt;. (Our &lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications"&gt;article introducing micropipenv&lt;/a&gt; includes an overview of these projects.)&lt;/p&gt; &lt;p&gt;The &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; is the primary index consulted by pip. In some cases, applications need libraries from other Python package indexes. For these, pip provides the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-index-url"&gt;--index-url&lt;/a&gt; and &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-extra-index-url"&gt;--extra-index-url&lt;/a&gt; options. Most of the time, there are two primary reasons you might need to install dependencies from Python package sources other than PyPI:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Installing specific builds of packages whose features cannot be expressed using wheel tags, or that do not meet &lt;a href="https://github.com/pypa/manylinux"&gt;manylinux standards&lt;/a&gt;; e.g., the &lt;a href="https://tensorflow.pypi.thoth-station.ninja/"&gt;AVX2-enabled builds of TensorFlow&lt;/a&gt; hosted on the Python package index of the Artificial Intelligence Center of Excellence (AICoE).&lt;/li&gt; &lt;li&gt;Installing packages that should not be hosted on PyPI, such as packages specific to one company or patched versions of libraries used only for testing.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why Python is vulnerable to dependency confusion attacks&lt;/h2&gt; &lt;p&gt;The pip options &lt;code&gt;--index-url&lt;/code&gt; and &lt;code&gt;--extra-index-url&lt;/code&gt; provide a way to specify alternate Python package indexes for resolving and installing Python packages. The first option, &lt;code&gt;--index-url&lt;/code&gt;, specifies the main Python package index for resolving Python packages, and defaults to PyPI. When you need a second package index, you can include the &lt;code&gt;--extra-index-url&lt;/code&gt; option as many times as needed. The resolution logic in pip first uses the main index, then, if the required package or version is not found there, it checks the secondary indexes.&lt;/p&gt; &lt;p&gt;Thus, although you can specify the order in which indexes are consulted, the configuration is not specified for each package individually. Moreover, the index configuration is applied for transitive dependencies introduced by direct dependencies, as well.&lt;/p&gt; &lt;p&gt;To bypass this order, application developers can manage requirements with hashes that are checked during installation and resolution to differentiate releases. This solution is unintuitive and error-prone, however. Although we encourage keeping hashes in lock files for integrity checks, they should be managed automatically using the appropriate tools.&lt;/p&gt; &lt;p&gt;Now, let’s imagine a dependency named &lt;code&gt;foo&lt;/code&gt; that a company uses on a private package index. Suppose a different package with the same name is hosted on PyPI. An unexpected glitch—such as a temporary network issue when resolving the company private package index—could lead the application to import the &lt;code&gt;foo&lt;/code&gt; package from PyPI in default setups. In the worst case, the package published on PyPI might be a &lt;a href="https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610"&gt;malicious alternative&lt;/a&gt; that reveals company secrets to an attacker.&lt;/p&gt; &lt;p&gt;This issue also applies to pip-tools, Pipenv, and Poetry). Pipenv provides a way to configure a Python package index for a specific package, but it does not enforce the specified configuration. All the mentioned dependency resolution tools treat multiple Python package indexes supplied as mirrors.&lt;/p&gt; &lt;h2&gt;Using Thoth to resolve dependency confusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://thoth-station.ninja/"&gt;Thoth&lt;/a&gt; is a project sponsored by Red Hat that takes a fresh look at the complex needs of Python applications and &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;moves the resolution process to the cloud&lt;/a&gt;. Naturally, being cloud-based has its advantages and disadvantages depending on how the tool is used.&lt;/p&gt; &lt;p&gt;Because Thoth moves dependency resolution to the cloud, a central authority can resolve application requirements. This central authority can be configured with fine-grained control over which application dependencies go into desired environments. For instance, you could handle dependencies in test environments and production environments differently.&lt;/p&gt; &lt;p&gt;Thoth's resolver pre-aggregates information about Python packages from various Python package indexes. This way, the resolver can monitor Python packages published on PyPI, on the AICoE-specific TensorFlow index, on a &lt;a href="https://www.operate-first.cloud/community-handbook/pulp/usage.md"&gt;corporate Pulp Python index&lt;/a&gt;, on the &lt;a href="https://download.pytorch.org/whl/cu111/"&gt;PyTorch CUDA 11.1 index&lt;/a&gt;, and on &lt;a href="https://download.pytorch.org/whl/cpu/"&gt;builds for CPU use&lt;/a&gt;, which the PyTorch community provides for specific cases. Moreover, the cloud-based resolver &lt;a href="https://thoth-station.ninja/docs/developers/adviser/security.html"&gt;introspects the published packages with respect to security&lt;/a&gt; or &lt;a href="https://github.com/thoth-station/cve-update-job"&gt;vulnerabilities&lt;/a&gt; (see &lt;a href="https://github.com/pypa/advisory-db"&gt;PyPA’s Python Packaging Advisory Database&lt;/a&gt;) to additionally guide a &lt;a href="https://developers.redhat.com/articles/2021/09/29/secure-your-python-applications-thoth-recommendations"&gt;secure resolution process&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;contact the Thoth team&lt;/a&gt; if you wish to register your own Python package index to Thoth.&lt;/p&gt; &lt;h3&gt;Solver rules in Thoth&lt;/h3&gt; &lt;p&gt;A central authority can be configured to allow or block packages or specific package releases that are hosted on the Python package indexes. This feature is called &lt;em&gt;solver rules&lt;/em&gt; and is maintained by a Thoth operator.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://thoth-station.ninja/docs/developers/adviser/deployment.html#configuring-solver-rules"&gt;Configuring solver rules&lt;/a&gt; in the Thoth documentation for more about this topic. Also check out our &lt;a href="https://www.youtube.com/watch?v=wjMNOyGupbs"&gt;YouTube video demonstrating solver rules&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use solver rules to allow the Thoth operator to specify which Python packages or specific releases can be considered during the resolution process, respecting the Python package indexes registered when a request is made to the cloud-based resolver. You can also use solver rules to block the analysis of packages that are considered too old, are no longer supported, or simply don't adhere to company policies.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;Report issues with open source Python packages&lt;/a&gt; to help us create new solver rules.&lt;/p&gt; &lt;h3&gt;Strict index configuration&lt;/h3&gt; &lt;p&gt;Another feature in Thoth is the ability to &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;configure a strict Python package index configuration&lt;/a&gt;. By default, the recommendation engine considers all the packages published on the indexes it monitors and uses a &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; to come up with a set of packages that are considered most appropriate. However, in some situations, Thoth users want to suppress this behavior and explicitly configure Python package indexes for consuming Python packages on their own.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are interested in the strict index configuration, please &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;browse the documentation&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=p6fjVQ0aUPE"&gt;watch our video demonstration&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prescriptions&lt;/h3&gt; &lt;p&gt;Thoth also supports a &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;mechanism called prescriptions&lt;/a&gt; that provides additional, detailed guidelines for package resolution. Prescriptions are analogous to manifests in Kubernetes and OpenShift. A manifest lists the desired state of the cluster, and the machinery behind the cluster orchestrator tries to create and maintain the desired state. Similarly, prescriptions provide a declarative way to specify the resolution process for the particular dependencies and Python package indexes used.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See the &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;prescriptions section&lt;/a&gt; in the Thoth documentation for more about this feature. You can also browse Thoth's &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt; for prescriptions available for open source Python projects. See our &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;article about prescriptions&lt;/a&gt; for more insight into this concept.&lt;/p&gt; &lt;p&gt;Thoth's &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; searches for a solution that satisfies application requirements, taking prescriptions into account. This algorithm provides the power to adjust the resolution process in whatever manner users desire. Adjustments to the resolution process can be made using &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription/should_include.html#should-include-labels"&gt;labeled requests to the resolver&lt;/a&gt; which can pick prescriptions that match specified criteria written in YAML files. An example can be consuming all the packages solely from one package index (such as a Python package index hosted using &lt;a href="https://pulpproject.org/"&gt;Pulp&lt;/a&gt;) that hosts packages that can be considered as trusted for Thoth users.&lt;/p&gt; &lt;h2&gt;About Project Thoth&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow project updates, please &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;subscribe to our YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation Twitter handle&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" title="Prevent Python dependency confusion attacks with Thoth"&gt;Prevent Python dependency confusion attacks with Thoth&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2021-12-21T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Architectural introduction</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-architetural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-architetural-introduction.html</id><updated>2021-12-21T06:00:00Z</updated><content type="html">Part 1 - Architectural introduction The last few months we have been digging deeply into the world of healthcare architectures with a focus on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in that we have the mission of creating architectural content based on common customer adoption patterns.  That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Portfolio Architecture.  Let's look at these architectures, how they're created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division into smaller parts. In this case presented here is we are looking closer at the healthcare industry and an intelligent data as a service (iDaaS) architecture. This use case we've defined as the following: iDaaS is all about transforming the way the healthcare industry interacts with data and information. It provides an example for connecting, processing and leveraging clinical, financial, administrative, and life sciences data at scale in a consistent manner.  The approach taken is to research our existing customers that have implemented solutions in this space, collect their public-facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architecture for use in telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on intelligent data as a service architecture: 1. 2. Common architectural elements 3. Example iDaaS architecture 4. Example HL7 and FHIR integration architecture 5. Example iDaaS knowledge and insight architecture Catch up on any past articles you missed by following any published links above. Next in this series, we will take a look at the generic common architectural elements for the intelligent data as a service architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">How to Integrate Keycloak for Authentication with Apache APISIX</title><link rel="alternate" href="https://www.keycloak.org/2021/12/apisix" /><author><name>Xinxin Zhu &amp; Yilin Zeng</name></author><id>https://www.keycloak.org/2021/12/apisix</id><updated>2021-12-21T00:00:00Z</updated><content type="html">This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in Apache APISIX through detailed steps. is an open source identity and access management solution for modern applications and services. Keycloak supports Single-Sign On, which enables services to interface with Keycloak through protocols such as OpenID Connect, OAuth 2.0, etc. Keycloak also supports integrations with different authentication services, such as Github, Google and Facebook. In addition, Keycloak also supports user federation, and can import users through LDAP and Kerberos. For more information about Keycloak, please refer to the . is a dynamic, real-time, high-performance API gateway, providing rich traffic management. The project offers load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and many useful plugins. In addition, the gateway supports dynamic plugin changes along with hot update. The OpenID Connect plugin for Apache APISIX allows users to replace traditional authentication mode with centralized identity authentication mode via OpenID Connect. HOW TO USE INSTALL APACHE APISIX INSTALL DEPENDENCIES The Apache APISIX runtime environment requires dependencies on NGINX and etcd. Before installing Apache APISIX, please install dependencies according to the operating system you are using. We provide the dependencies installation instructions for CentOS7, Fedora 31 and 32, Ubuntu 16.04 and 18.04, Debian 9 and 10, and macOS. Please refer to for more details. INSTALLATION VIA RPM PACKAGE (CENTOS 7) This installation method is suitable for CentOS 7; please run the following command to install Apache APISIX. sudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm INSTALLATION VIA DOCKER Please refer to . INSTALLATION VIA HELM CHART Please refer to . INITIALIZING DEPENDENCIES Run the following command to initialize the NGINX configuration file and etcd. make init START APACHE APISIX Run the following command to start Apache APISIX. apisix start START KEYCLOAK Here we use docker to start Keycloak. docker run -p 8080:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=password -e DB_VENDOR=h2 -d jboss/keycloak:9.0.2 After execution, you need to verify that Keycloak have started successfully. docker ps CONFIGURE KEYCLOAK After Keycloak is started, use your browser to access "http://127.0.0.1:8080/auth/admin/" and type the admin/password account password to log in to the administrator console. CREATE A REALM First, you need to create a realm named apisix_test_realm. In Keycloak, a realm is a workspace dedicated to managing projects, and the resources of different realms are isolated from each other. The realm in Keycloak is divided into two categories: one is the master realm, which is created when Keycloak is first started and used to manage the admin account and create other realm. the second is the other realm, which is created by the admin in the master realm and can be used to create, manage and use users and applications in this realm. The second category is the other realm, created by admin in the master realm, where users and applications can be created, managed and used. For more details, please refer to the . CREATE A CLIENT The next step is to create the OpenID Connect Client. In Keycloak, Client means a client that is allowed to initiate authentication to Keycloak. In this example scenario, Apache APISIX is equivalent to a client that is responsible for initiating authentication requests to Keycloak, so we create a Client with the name apisix. More details about the Client can be found in . CONFIGURE THE CLIENT After the Client is created, you need to configure the Apache APISIX access type for the Client. In Keycloak, there are three types of Access Type: 1. Confidential: which is used for applications that need to perform browser login, and the client will get the access token through client secret, mostly used in web systems rendered by the server. 2. Public: for applications that need to perform browser login, mostly used in front-end projects implemented using vue and react. 3. Bearer-only: for applications that don’t need to perform browser login, only allow access with bearer token, mostly used in RESTful API scenarios. For more details about Client settings, please refer to . Since we are using Apache APISIX as the Client on the server side, we can choose either "Confidential" Access Type or "Bearer-only" Access Type. For the demonstration below, we are using "Confidential" Access Type as an example. CREATE USERS Keycloak supports interfacing with other third-party user systems, such as Google and Facebook, or importing or manually creating users using LDAP . Here we will use "manually creating users" to demonstrate. Then set the user’s password in the Credentials page. CREATE ROUTES After Keycloak is configured, you need to create a route and open the Openid-Connect plugin . For details on the configuration of this plugin, please refer to the . GET CLIENT_ID AND CLIENT_SECRET In the above configuration. * client_id is the name used when creating the Client before, i.e. apisix * client_secret should be obtained from Clients-apisix-Credentials, for example: d5c42c50-3e71-4bbbe-aa9e-31083ab29da4. GET THE DISCOVERY CONFIGURATION Go to Realm Settings-General-Endpoints, select the OpenID Endpoint Configuration link and copy the address that the link points to, for example:`http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration`. CREATE A ROUTE AND ENABLE THE PLUG-IN Use the following command to access the Apache APISIX Admin interface to create a route, set the upstream to httpbin.org, and enable the plug-in OpenID Connect for authentication. Note: If you select bearer-only as the Access Type when creating a Client, you need to set bearer_only to true when configuring the route, so that access to Apache APISIX will not jump to the Keycloak login screen. curl -XPOST 127.0.0.1:9080/apisix/admin/routes -H "X-Api-Key: edd1c9f034335f136f87ad84b625c8f1" -d '{ "uri":"/*", "plugins":{ "openid-connect":{ "client_id":"apisix", "client_secret":"d5c42c50-3e71-4bbe-aa9e-31083ab29da4", "discovery":"http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration", "scope":"openid profile", "bearer_only":false, "realm":"apisix_test_realm", "introspection_endpoint_auth_method":"client_secret_post", "redirect_uri":"http://127.0.0.1:9080/" } }, "upstream":{ "type":"roundrobin", "nodes":{ "httpbin.org:80":1 } } }' ACCESS TESTING Once the above configuration is complete, we are ready to perform the relevant access tests in Apache APISIX. ACCESS APACHE APISIX Use your browser to access . Since the OpenID-Connect plugin is enabled and bearer-only is set to false, when you access this path for the first time, Apache APISIX will redirect to the login screen configured in apisix_test_realm in Keycloak and make a user login request. Enter the User peter created during the Keycloak configuration to complete user login. SUCCESSFUL ACCESS After a successful login, the browser will again redirect the link to and will successfully access the image content. The content is identical to that of the upstream . LOGOUT After the test, use your browser to access http:/127.0.0.1:9080/logout to logout your account. Note: The logout path can be specified by logout_path in the OpenID-Connect plug-in configuration, the default is logout. SUMMARY This article shows the procedure of using OpenID-Connect protocol and Keycloak for authentication in Apache APISIX. By integrating with Keycloak, Apache APISIX can be configured to authenticate and authenticate users and application services, which greatly reduces the development work involved. For more information about the implementation of authentication in Apache APISIX, see .</content><dc:creator>Xinxin Zhu &amp; Yilin Zeng</dc:creator></entry></feed>
